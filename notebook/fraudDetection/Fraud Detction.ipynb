{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Fraud Detection demo:\n",
    "\n",
    "The notebook demonstrates how to conduct a fraud detection application with the BigDL deep learning library on Apache Spark. We'll try to introduce some techniques that can be used for training a fraud detection model, but some advanced skills is not applicable since the dataset is highly simplified.\n",
    "\n",
    "**Dataset:**\n",
    "Credit Card Fraud Detection\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "**Software stack:**\n",
    "Scala 2.11 + Spark 2.1 + BigDL Master\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from csv files and output the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 284807                                                                   \n",
      "root\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Time: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, MultilayerPerceptronClassifier, _}\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
    "import org.apache.spark.ml.feature.{MinMaxScaler, StandardScaler, VectorAssembler}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val conf = Engine.createSparkConf()\n",
    "val spark = SparkSession.builder().master(\"local[1]\").appName(\"ss\").config(conf).getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "val raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv(\"data/creditcard.csv\")\n",
    "val df = raw.select(((1 to 28).map(i => \"V\" + i) ++ Array(\"Time\", \"Amount\", \"Class\")).map(s => col(s).cast(\"Double\")): _*)\n",
    "\n",
    "println(\"total: \" + df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature analysis:**\n",
    "\n",
    "Normally it would improve the model if we could derive more features from the raw transaction records. E.g.\n",
    "    days to last transaction,\n",
    "    distance with last transaction,\n",
    "    amount percentage over the last 1 month / 3months\n",
    "    ...\n",
    "\n",
    "Yet with the public dataset, we can hardly derive any extention features from the PCA result. So here we only introduce several general practices:\n",
    "1. Usually there's a lot of categorical data in the raw dataset, E.g. post code, card type, merchandise id, seller id, etc.\n",
    "    1). For categorical feature with limited candidate values, like card type, channel id, just use OneHotEncoder.\n",
    "    2). For categorical feature with many candidate values, like merchandise id, post code or even phone number, suggest to use Weight of Evidence.\n",
    "    3). You can also use FeatureHasher from Spark MLlib which will be release with Spark 2.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, essentially it's only a classification training problem with highly unbalanced data set.\n",
    "\n",
    "** Approach **\n",
    "1. We will build a feature transform pipeline with Apache Spark and some of our transformers.\n",
    "2. We will run some inital statistical analysis and split the dataset for training and validation.\n",
    "3. We will build the model with BigDL.\n",
    "4. We will compare different strategy to handle the unbalance.\n",
    "4. We will evaluate the model with grid search and area under precision-recall curve.\n",
    "\n",
    "Details of each step is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***step 1. Build an inital pipeline for feature transform.***\n",
    "\n",
    "For each training records, we intend to aggregate all the features into one Spark Vector, which will then be sent to BigDL model for the training. First we'd like to introduce one handy transformer that we developed to help user build custom Transformers for Spark ML Pipeline. \n",
    "    \n",
    "    \n",
    "    ```\n",
    "    class FuncTransformer (\n",
    "      override val uid: String,\n",
    "      val func: UserDefinedFunction\n",
    "    ) extends Transformer with HasInputCol with HasOutputCol with DefaultParamsWritable {\n",
    "    ```\n",
    "`FuncTransformer` takes an udf as the constructor parameter and use the udf to perform the actual transform. The transformer can be saved/loaded as other transformer and can be integrated into a pipeline normally. It can be used widely in many use cases like conditional conversion(if...else...), , type conversion, to/from Array, to/from Vector and many string ops.\n",
    "Some examples: \n",
    "`val labelConverter = new FuncTransformer(udf { i: Double => if (i >= 1) 1 else 0 })`\n",
    "    \n",
    "`val shifter = new FuncTransformer(udf { i: Double => i + 1 })`\n",
    "    \n",
    "`val toVector = new FuncTransformer(udf { i: Double => Vectors.dense(i) })`\n",
    "    \n",
    "We will use `VectorAssembler` to compose the all the Vx columns and append the Amount column. Then use `StandardScaler` to normlize the training records. Since in BigDL, the criterion generally only accepts 1, 2, 3... as the Label, so we will replace all the 0 with 2 in the training data.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 15:=======>                                                  (1 + 7) / 8]+--------------------+-----+\n",
      "|            features|Class|\n",
      "+--------------------+-----+\n",
      "|[-0.6942411021638...|  2.0|\n",
      "|[0.60849525943109...|  2.0|\n",
      "|[-0.6934992452238...|  2.0|\n",
      "|[-0.4933240320774...|  2.0|\n",
      "|[-0.5913287255806...|  2.0|\n",
      "|[-0.2174742415711...|  2.0|\n",
      "|[0.62779408220828...|  2.0|\n",
      "|[-0.3289277697329...|  2.0|\n",
      "|[-0.4565722152677...|  2.0|\n",
      "|[-0.1726974406947...|  2.0|\n",
      "|[0.73980031932134...|  2.0|\n",
      "|[0.19654824114227...|  2.0|\n",
      "|[0.63817910856358...|  2.0|\n",
      "|[0.54596205586179...|  2.0|\n",
      "|[-1.4253641430361...|  2.0|\n",
      "|[-0.3841418567777...|  2.0|\n",
      "|[0.56323980125506...|  2.0|\n",
      "|[-0.2230591756515...|  2.0|\n",
      "|[-2.7575786155874...|  2.0|\n",
      "|[0.76220920780347...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import com.intel.analytics.bigdl.nn._\n",
    "import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n",
    "import org.apache.spark.ml.feature.{FuncTransformer, MinMaxScaler, StandardScaler, VectorAssembler}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val labelConverter = new FuncTransformer(udf {d: Double => if (d==0) 2 else d }).setInputCol(\"Class\").setOutputCol(\"Class\")\n",
    "val assembler = new VectorAssembler().setInputCols((1 to 28).map(i => \"V\" + i).toArray ++ Array(\"Amount\")).setOutputCol(\"assembled\")\n",
    "val scaler = new StandardScaler().setInputCol(\"assembled\").setOutputCol(\"features\")\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, scaler, labelConverter))\n",
    "val pipelineModel = pipeline.fit(df)\n",
    "val data = pipelineModel.transform(df)\n",
    "data.select(\"features\", \"Class\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***step 2. split the dataset into training and validation dataset.***\n",
    "\n",
    "Unlike some other training dataset, where the data does not have a time of occurance. For this case, we can know the sequence of the transactions from the Time column. Thus randomly splitting the data into training and validation does not make much sense, since in real world applications, we can only use the history transactions for training and use the latest transactions for validation. Thus we'll split the dataset according the time of occurance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training count: 199424                                                          \n",
      "validData count: 85383                                                          \n"
     ]
    }
   ],
   "source": [
    "    val splitTime = data.stat.approxQuantile(\"Time\", Array(0.7), 0.001).head\n",
    "\n",
    "    val trainingData = data.filter(s\"Time<$splitTime\").cache()\n",
    "    val validData = data.filter(s\"Time>=$splitTime\").cache()\n",
    "    println(\"training count: \" + trainingData.count())\n",
    "    println(\"validData count: \" + validData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***step 3. Build the model with BigDL***\n",
    "\n",
    "From the research community and industry feedback, a simple neural network turns out be the perfect candidate for the fraud detection training. We will quickly build a multiple layer Perceptron with linear layers.\n",
    "```\n",
    "    val bigDLModel = Sequential()\n",
    "      .add(Linear(29, 10))\n",
    "      .add(Linear(10, 2))\n",
    "      .add(LogSoftMax())\n",
    "    val criterion = ClassNLLCriterion()\n",
    "      ```\n",
    "BigDL provides `DLEstimator` and `DLClassifier` for users with Apache Spark MLlib experience, which\n",
    "provides high level API for training a BigDL Model with the Apache Spark `Estimator`/`Transfomer`\n",
    "pattern, thus users can conveniently fit BigDL into a ML pipeline. The fitted model `DLModel` and\n",
    "`DLClassiferModel` contains the trained BigDL model and extends the Spark ML `Model` class.\n",
    "Alternatively users may also construct a `DLModel` with a pre-trained BigDL model to use it in\n",
    "Spark ML Pipeline for prediction.\n",
    "\n",
    "`DLClassifier` is a specialized `DLEstimator` that simplifies the data format for\n",
    "classification tasks. It only supports label column of DoubleType, and the fitted\n",
    "`DLClassifierModel` will have the prediction column of DoubleType.\n",
    "\n",
    "For this case we'll just use `DLClassifier` for the training. Note that users can set differet optimization mothod, batch size and epoch number.     \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import com.intel.analytics.bigdl.nn._\n",
    "import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.utils.Engine\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.ml.ensemble.Bagging\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
    "import org.apache.spark.ml.feature.{FuncTransformer, MinMaxScaler, StandardScaler, VectorAssembler}\n",
    "import org.apache.spark.ml.{DLClassifier, Pipeline}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "val bigDLModel = Sequential().add(Linear(29, 10)).add(Linear(10, 2)).add(LogSoftMax())\n",
    "val criterion = ClassNLLCriterion()\n",
    "val dlClassifier = new DLClassifier(bigDLModel, criterion, Array(29)).setLabelCol(\"Class\").setBatchSize(trainingData.count().toInt).setMaxEpoch(200)\n",
    "val model = dlClassifier.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    val labelConverter2 = new FuncTransformer(udf {d: Double => if (d==2) 0 else d }).setInputCol(\"Class\").setOutputCol(\"Class\")\n",
    "    val labelConverter3 = new FuncTransformer(udf {d: Double => if (d==2) 0 else d }).setInputCol(\"prediction\").setOutputCol(\"prediction\")\n",
    "    val finalData = labelConverter2.transform(labelConverter3.transform(model.transform(validData)))\n",
    "    \n",
    "    val metrics = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Class\")\n",
    "    val auPRC = metrics.evaluate(prediction)\n",
    "    println(\"Area under precision-recall curve = \" + auPRC)\n",
    "    \n",
    "    val recall = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"Class\")\n",
    "      .setMetricName(\"weightedRecall\")\n",
    "      .evaluate(prediction)\n",
    "    println(\"recall = \" + recall)\n",
    "\n",
    "    val precisoin = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"Class\")\n",
    "      .setMetricName(\"weightedPrecision\")\n",
    "      .evaluate(prediction)\n",
    "    println(\"Precision = \" + precisoin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Area under precision-recall curve = 0.75\n",
    "\n",
    "recall = 0.9995456054806969\n",
    "Precision = 0.9995236507066619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, we have finished the training and evaluation with a simple BigDL model. Next we'll try to optimize the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***step 4. handle the data imbalance***\n",
    "\n",
    "There are several ways to approach this classification problem taking into consideration this unbalance.\n",
    "1. Collect more data? Nice strategy but not applicable in this case\n",
    "2. Resampling the dataset\n",
    "    Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n",
    "    One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n",
    "    Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)\n",
    "3. Apart from under and over sampling, there is a very popular approach called SMOTE (Synthetic Minority Over-Sampling Technique), which is a combination of oversampling and undersampling, but the oversampling approach is not by replicating minority class but constructing new minority class data instance via an algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll start with Resampling.\n",
    "\n",
    "Since there're 492 frauds out of 284,807 transactions, to build a reasonable training dataset, we'll use UNDER-sampling for normal transactions and use OVER-sampling for fraud transactions. By using the sampling rate as \n",
    "fraud -> 10, normal -> 0.05, we can get a training dataset of (5K fraud + 14K normal) transactions. We can use the training data to fit a model.\n",
    "\n",
    "Yet we'll soon find that since there're only 5% of all the normal transactions are included in the training data, the model can only cover 5% of all the normal transactions, which is obviousely not optimistic. So how can we get a better converage for the normal transactions without breaking the ideal ratio in the training dataset?\n",
    "\n",
    "An immediate improvement would be to train multiple models. For each model, we will run the resampling from the original dataset and get a new training data set. After training, we can select best voting strategy for all the models to make the prediction.\n",
    "\n",
    "We'll use Ensembling of neural networks. That's where a Bagging classifier comes handy. Bagging is an Estimator we developed for ensembling of multiple other Estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package org.apache.spark.ml.ensemble\n",
    "\n",
    "class Bagging[M <: Model[M]](override val uid: String)\n",
    "  extends Estimator[BaggingModel[M]]\n",
    "  with BaggingParams[M] {\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "For usage, user need to set the specific Estimator to use and the number of models to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    val estimator = new Bagging()\n",
    "      .setPredictor(dlClassifier)\n",
    "      .setLabelCol(\"Class\")\n",
    "      .setIsClassifier(true)\n",
    "      .setNumModels(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Bagging will train $(numModels) models. Each model is trained with the resampled data from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    val models = (0 until $(numModels)).map { _ =>\n",
    "      val sampler = new StratifiedSampler(Map(2 -> 0.05, 1-> 10, 0 -> 1)).setLabel(\"Class\")\n",
    "      val bootstrapSample = sampler.transform(dataset)\n",
    "      val oldclassifier = $(predictor).asInstanceOf[DLClassifier[Float]]\n",
    "      val dlClassifier = new DLClassifier(oldclassifier.model, oldclassifier.criterion, Array(29)).setLabelCol(\"Class\")\n",
    "        .setBatchSize(oldclassifier.getBatchSize)\n",
    "        .setMaxEpoch(oldclassifier.getMaxEpoch)\n",
    "      dlClassifier.fit(bootstrapSample).asInstanceOf[M]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can tune the voting strategy via `model.setThreshold(t)`. If using Threshold = 5, we can get the improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Area under precision-recall curve = 0.9295288238964008\n",
    "[Stage 8428:============================>                           (1 + 1) / 2]recall = 0.9922262595206485\n",
    "[Stage 8434:============================>                           (1 + 1) / 2]Precision = 0.998684143354997"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
