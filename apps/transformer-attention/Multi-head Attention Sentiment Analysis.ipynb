{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the Transformer network architecture based solely on attention mechanisms. \n",
    "\n",
    "The idea comes from the https://arxiv.org/abs/1706.03762 “Attention is All You Need”, an influential paper with a catchy title that brings innovative change in the field of machine translation. This paper demonstrated how high performance can be achieved __without__ convolutional or recurrent neural networks, which were previously regarded as the go-to architecture for machine translation.\n",
    "\n",
    "In this notebook, we show that the Multi-head Attention network architecture can be implemented with Zoo Keras API and resolve the sentimental analysis task with IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from zoo.pipeline.api.keras.models import Model\n",
    "from zoo.pipeline.api.keras.layers import *\n",
    "from zoo.pipeline.api.autograd import *\n",
    "from zoo.common.nncontext import *\n",
    "\n",
    "sc = init_nncontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation\n",
    "\n",
    "Load 25000 records as training data, and another 25000 records as validation dataset. Each record in `x_train` and `x_test` represents one text comment from IMDB movie comments, where the sentences have been encoded with the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "max_len = 200\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention Layer\n",
    "\n",
    "The attention mechanism in the Transformer is interpreted as a way of computing the relevance of a set of values(information) based on some keys and queries. Basically, the attention mechanism is used as a way for the model to focus on relevant information based on what it is currently processing. This is a natural extension of the original attention mechanism:  traditionally, the attention weights were the relevance of the encoder hidden states (values) in processing the decoder state (query) and were calculated based on the encoder hidden states (keys) and the decoder hidden state (query).\n",
    "\n",
    "To improve the expressive ability of the model, the Transformer uses the Multi-Head Attention block. Instead of computing a single attention pass over the values, the Multi-Head Attention computes multiple attention weighted sums – hence the name “Multi-Head” Attention.\n",
    "\n",
    "To learn diverse representations, the Multi-Head Attention applies different linear transformations to the values, keys, and queries for each “head” of attention. This is illustrated in the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_multi_head_attention_model(q, k, v, n_head, d_k, d_v, d_out):\n",
    "    '''\n",
    "    Build a multi-Head Attention layer (https://arxiv.org/abs/1706.03762) with existing Keras Layers.\n",
    "    The implementation can leverage more matrix operation after Batch_Dot in Zoo supports dimension > 3.\n",
    "    :param q: Input Node for query\n",
    "    :param k: Input Node for key\n",
    "    :param v: Input Node for value\n",
    "    :param n_head: numner of heads\n",
    "    :param d_k: size of output for the linear transformation of key\n",
    "    :param d_v: size of output for the linear transformation of value\n",
    "    :param d_out: size of output for the linear transformation of the multihead.\n",
    "    :return:\n",
    "    '''\n",
    "    qs_layers = []\n",
    "    ks_layers = []\n",
    "    vs_layers = []\n",
    "    for _ in range(n_head):\n",
    "        qs_layers.append(TimeDistributed(Dense(d_k, bias=False)))\n",
    "        ks_layers.append(TimeDistributed(Dense(d_k, bias=False)))\n",
    "        vs_layers.append(TimeDistributed(Dense(d_v, bias=False)))\n",
    "\n",
    "    heads = []\n",
    "    for i in range(n_head):\n",
    "        qs = qs_layers[i](q)\n",
    "        ks = ks_layers[i](k)\n",
    "        vs = vs_layers[i](v)\n",
    "        attn = Lambda(lambda x, y: batch_dot(x, y, axes=[2, 2]) / np.sqrt(d_k))([qs, ks])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        head = Lambda(lambda x, y: batch_dot(x, y, axes=[2, 1]))([attn, vs])\n",
    "        heads.append(head)\n",
    "\n",
    "    head = merge(heads, mode=\"concat\")\n",
    "    output = TimeDistributed(Dense(d_out, activation = \"relu\"))(head)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition for Sentiment Analysis.\n",
    "\n",
    "Now we use the Multi-head Attention component to build a text classification model for sentimental analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasInput\n",
      "creating: createZooKerasEmbedding\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasVariable\n",
      "creating: createZooKerasLambdaLayer\n",
      "creating: createZooKerasMerge\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasGlobalAveragePooling1D\n",
      "creating: createZooKerasDropout\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasModel\n"
     ]
    }
   ],
   "source": [
    "S_inputs = Input(shape=(max_len,))\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "O_seq = build_multi_head_attention_model(embeddings, embeddings, embeddings, 8, 16, 16, 16)\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.2)(O_seq)\n",
    "outputs = Dense(2, activation='softmax')(O_seq)\n",
    "\n",
    "model = Model(S_inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAdam\n",
      "creating: createZooKerasSparseCategoricalCrossEntropy\n",
      "creating: createZooKerasAccuracy\n",
      "Train...\n",
      "Train finished.\n"
     ]
    }
   ],
   "source": [
    "# Users may try use different optimizers and different optimizer configs\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 40\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=1)\n",
    "print(\"Train finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Evaluated result: 0.871999979019165, total_num: 25000, method: Top1Accuracy\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating...')\n",
    "score = model.evaluate(x_test, y_test)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "With the Multi-head Attention architect, we can get 87% accuracy on the validation dataset. While Transformer network architect is more suitable for seq2seq tasks like translation, we use this simple example to demo how to build a multi-head attention model with the existing Zoo Keras API and apply it to a real world problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
